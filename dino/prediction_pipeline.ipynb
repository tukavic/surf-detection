{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db75e3d-a19e-49f5-a5bb-b7695207d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import blob_dog, blob_doh, blob_log\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "import vision_transformer as vits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369bff1-11da-4846-84ad-17e5bbc9014a",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41601925-d221-4f0d-84fc-951da65ef5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'vit_small'\n",
    "patch_size = 8\n",
    "pretrained_weights = 'vit_small'\n",
    "checkpoint_key = None\n",
    "image_path = 'surf.png'\n",
    "\n",
    "image_size = (592, 1184)\n",
    "output_dir = '.'\n",
    "threshold = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea286ca-ffce-4519-b210-49e3f4979cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# build model\n",
    "model = vits.__dict__[arch](patch_size=patch_size, num_classes=0)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if os.path.isfile(pretrained_weights):\n",
    "    state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "    if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "        print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "        state_dict = state_dict[checkpoint_key]\n",
    "    # remove `module.` prefix\n",
    "    state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    # remove `backbone.` prefix induced by multicrop wrapper\n",
    "    state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))\n",
    "else:\n",
    "    print(\"Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\")\n",
    "    url = None\n",
    "    if arch == \"vit_small\" and patch_size == 16:\n",
    "        url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "    elif arch == \"vit_small\" and patch_size == 8:\n",
    "        url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"  # model used for visualizations in our paper\n",
    "    elif arch == \"vit_base\" and patch_size == 16:\n",
    "        url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
    "    elif arch == \"vit_base\" and patch_size == 8:\n",
    "        url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
    "    if url is not None:\n",
    "        print(\"Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\")\n",
    "        state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "    else:\n",
    "        print(\"There is no reference weights available for this model => We use random weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c761e35-b2c0-453a-9fe8-69c774c1bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file, image_size):\n",
    "    '''loads picture and applies torch transforms'''\n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    transform = pth_transforms.Compose([\n",
    "        pth_transforms.Resize(image_size), # Do we need this?\n",
    "        pth_transforms.ToTensor(),\n",
    "        pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f92aeff-f989-4fbe-bbb7-299f3c8e5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_image(img, patch_size):\n",
    "    '''Reshapes image to be divisible by the patch size'''\n",
    "    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n",
    "    img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "    w_featmap = img.shape[-2] // patch_size\n",
    "    h_featmap = img.shape[-1] // patch_size\n",
    "    \n",
    "    return img, w_featmap, h_featmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2abc10-c5fc-4cbe-b805-2f5768b9088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_image(model, img, patch_size):\n",
    "    '''Passes image through model and returns attention mask'''\n",
    "    img, w_featmap, h_featmap = patch_image(img, patch_size)\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n",
    "    nh = attentions.shape[1] # number of head\n",
    "\n",
    "    # we keep only the output patch attention\n",
    "    heatmap = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "    heatmap = heatmap.reshape(nh, w_featmap, h_featmap)\n",
    "    \n",
    "    # interpolate attention mask back to original image size\n",
    "    heatmap = nn.functional.interpolate(heatmap.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n",
    "    \n",
    "    return heatmap.sum(0).astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed055aa-6ad8-4e53-8029-9957321a0804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_blobs(heatimg):\n",
    "    '''\n",
    "    predicts the number of surfers as the number of \n",
    "    blobs found in the attention mask of an image\n",
    "    '''\n",
    "    blobs = blob_doh(heatimg*256, threshold=0.05, min_sigma = 10, max_sigma=50)\n",
    "    \n",
    "    return len(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3163567-dadf-4220-bbbc-702bff9bfad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, image_size=(592, 1184), patch_size=8):\n",
    "    img = load_img(row['filename'], image_size)\n",
    "    global model\n",
    "    heatimg = mask_image(model, img, patch_size)\n",
    "    num_blobs = get_num_blobs(heatimg)\n",
    "    row['pred_num_surfers'] = num_blobs\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e5a614-1286-4f70-a0bd-d2e9ccaa290f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bde79bcd7f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get predictions for each image like this?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Get predictions for each image like this?\n",
    "df = df.apply(lambda row: predict(row), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
